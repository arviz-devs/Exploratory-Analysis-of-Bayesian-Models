{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes factors\n",
    "\n",
    "Bayes factors are a common way to compare models and perform hypothesis testing. They are also a subject that generally acts as a dividing line between Bayesians: We have those who use Bayes factors and those who dislike them. I mean, I bet there are people in the middle too, but more often than not this is a polarizing topic.\n",
    "\n",
    "To understand Bayes factors let us begin by writing Bayes theorem in a way that explicitly shows that our inferences are always model dependent.\n",
    "\n",
    "$$p(\\theta \\mid y, M_k) = {\\frac {p(y \\mid \\theta, M_k)p(\\theta \\mid M_k)}{p(y \\mid M_k)}}$$\n",
    "\n",
    "Where $y$ represents the data and $M$ the model:\n",
    "\n",
    "The term in the denominator is known as marginal likelihood (or evidence). When doing inference we do not need to compute this normalizing constant, so in practice we often compute the posterior up to a constant factor. However, for model comparison and model averaging the marginal likelihood becomes a relevant quantity. If our main objective is to choose only one model, the best one, from a set of $k$ models we can just choose the one with the largest $p(y \\mid M_k)$. As a general rule $p(y \\mid M_k)$ are tiny numbers and do not tell us too much on their own; what matters are the relative values. So in practice people often compute the ratio of two marginal likelihoods, and this is called a Bayes factor:\n",
    "\n",
    "\n",
    "$$BF = \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}$$\n",
    "\n",
    "Then when BF > 1, model 0 explains data better than model 1.\n",
    "\n",
    "Some authors have proposed tables with ranges to discretize and ease BF interpretation. The following table indicates the strength of the evidence favoring model 0 against model 1:\n",
    "\n",
    "* 1-3: anecdotal\n",
    "* 3-10: moderate\n",
    "* 10-30: strong\n",
    "* 30-100: very strong\n",
    "* \\> 100: extreme\n",
    "\n",
    "Remember, these rules are just conventions, simple guides at best. But results should always be put into context and should be accompanied with enough detail so others could potentially check if they agree with our conclusions. The strength of the evidence necessary to make a claim is not the same in particle physics, a court, or to evacuate a town and prevent hundreds of deaths.\n",
    "\n",
    "Using $p(y \\mid M_k)$ to compare model is totally fine if all models are assumed to have the same prior probability[<sup>1</sup>](#fn1). Otherwise, we have to compute the *posterior odds*:\n",
    "\n",
    "$$\\underbrace{\\frac{p(M_0 \\mid y)}{p(M_1 \\mid y)}}_\\text{posterior odds} = \\underbrace{\\frac{p(y \\mid M_0)}{p(y \\mid M_1)}}_\\text{Bayes factors} \\, \\underbrace{\\frac{p(\\ M_0 \\ )}{p(\\ M_1 \\ )}}_\\text{prior odds}$$\n",
    "\n",
    "<span id=\"fn1\"> <sup>1</sup> Notice that we are talking about the prior probability we assign to models and not about the priors we assign to parameters for each model</span>\n",
    "\n",
    "\n",
    "## All that glitter is not gold\n",
    "\n",
    "By carefully inspecting the definition of marginal likelihood we can understand their properties and consequences for their practical use:\n",
    "\n",
    "$$p(y \\mid M_k) = \\int_{\\theta_k} p(y \\mid \\theta_k, M_k) p(\\theta_k, M_k) d\\theta_k$$\n",
    "\n",
    "\n",
    "* **The good: Models with more parameters have a larger penalization than models with fewer parameters**. Bayes factors has a built-in Occam Razor! The intuitive reason is that the larger the number of parameters the more *spread-out* the prior will be with respect to the likelihood. Or in other words a more *spread out* prior is one that admits as plausible more datasets than a more concentrated one. This will be reflected in the computation of the above integral as you will get a smaller value with a wider prior than with a more concentrated prior.  \n",
    "\n",
    "* **The bad: Computing the marginal likelihood is, generally, a hard task**. The above is an integral of a highly variable function over a high dimensional parameter space. In general this integral needs to be solved numerically using more or less sophisticated methods (see and example [here](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/Bayes_factor.html?highlight=bayes%20factor)).\n",
    "\n",
    "\n",
    "* **The ugly: The marginal likelihood depends *sensitively* on the values of the priors**. Using the marginal likelihood to compare models is a good idea because a penalization for complex models is already included (thus preventing us from overfitting). At the same time, a change in the prior will affect the computations of the marginal likelihood. At first this sounds a little bit silly: we already know that priors affect computations, otherwise we could simply avoid them, but the point here is the word \"sensitively\". We are talking about changes in the prior that will keep inference of the parameters virtually unchanged, but could have a big impact on the value of the marginal likelihood. Another source of criticism to Bayes factors is that they can be used as a Bayesian way of doing hypothesis testing; there is  nothing wrong with this *per se*, but many authors have pointed out that an inference approach is better suited to most problems than the generally taught approach of hypothesis testing (whether  Bayesian or not Bayesian).\n",
    "\n",
    "## Bayes Factors vs Information Criteria\n",
    "\n",
    "LOO and WAIC computations are based on the log-likelihood values, and the priors are not directly part of the computations -- they are indirectly included because they will have an effect on the posterior. Instead, Bayes factors use priors directly as we need to average the likelihood over the prior. Conceptually we can say that Bayes factors are focused on identifying the best model (and the prior is part of the model) while LOO and WAIC are focused on which parameters will give the best predictions. In most scenarios priors are used for their regularizing properties and, when possible, to provide some background knowledge -- more so than because we *really really believe* they reflect some *truth*. As a result, we think WAIC and LOO are better choices in practice.Both, LOO and WAIC computation is far less problematic and it does not requiere the use of a special samplers or methods. Additionally LOO is more robust in practice, it has a built-in diagnostic and can be used not only for model comparison but model criticism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
