

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Model Comparison &#8212; Exploratory analysis of Bayesian models</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Exploratory analysis of Bayesian models</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Section_04/Model_selection.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#predictive-accuracy" class="nav-link">Predictive accuracy</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#akaike-information-criterion" class="nav-link">Akaike Information Criterion</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#widely-available-information-criterion" class="nav-link">Widely available information criterion</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation" class="nav-link">Pareto smoothed importance sampling leave-one-out cross validation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-compare-function" class="nav-link">The compare function</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-plot-compare-function" class="nav-link">The plot_compare function</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#point-wise-model-comparison" class="nav-link">Point-wise model comparison</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>

<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-comparison">
<h1>Model Comparison<a class="headerlink" href="#model-comparison" title="Permalink to this headline">¶</a></h1>
<p>To perform a Bayesian analysis we need to define a model (a combination of prior and likelihood) and then apply Bayes theorem. In a magical world we should be able to define the <em>true</em> model corresponding to the <em>true</em> data generating process. In the real world, we can only hope to define a reasonable enough model to make predictions and/or explain the data. The process of finding this useful model is generally iterative and involves creating more than one candidate model.</p>
<p>When faced with more than one model for the same data it is <em>natural</em> to ask how these models compare to each other. One way to do this is to perform posterior predictive checks as we saw in the previous chapter. Another one is to evaluate the accuracy of the predictions on new data, <em>i.e.</em> data not used to fit the model in the first place. It is generally assumed that both datasets come from the <em>the true generating process</em>.</p>
<p>The inconvenience of using new data, is that in general we can not afford the luxury of putting aside a portion of our data and not use it to fit a model. Even in a <em>big-data regime</em> we have good reasons to use all the available data, two of them are:</p>
<ul class="simple">
<li><p>Reduce the uncertainty of our estimates</p></li>
<li><p>Increase the number of questions we can ask from the data.</p></li>
</ul>
<p>For this reason a number of strategies have been develop in order to evaluate the accuracy of predictions using just the same data used to fit the model, as we will see next.</p>
<div class="section" id="predictive-accuracy">
<h2>Predictive accuracy<a class="headerlink" href="#predictive-accuracy" title="Permalink to this headline">¶</a></h2>
<p>Ideally, any measure of predictive accuracy should take into account the application at hand and it should include benefits and cost of the model’s predictions. We discuss such an example in the “chapter XXX Decision-theory”. In this chapter we will, instead, discuss very general methods that are applicable to a wide range of models and problems.</p>
<p>A pretty common way of measuring how well a model fits the data is to compute the quadratic mean error between a data-point (<span class="math notranslate nohighlight">\(y_i\)</span>) and a pointwise prediction <span class="math notranslate nohighlight">\(\operatorname{E} (y_i \mid \theta))\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n} \sum_{i=1}^{n}  (y_i - \operatorname{E} (y_i \mid \theta))^2\]</div>
<p>This is the average of the quadratic differences between observed and predicted data. By taking the square we ensure that (positive and negative) errors do not cancel each other out. Also by taking the square we penalize larger deviations, compared to using for example the absolute value of the differences</p>
<p>When doing probabilistic forecasting measures of predictive accuracy are generally known as <a class="reference external" href="https://en.wikipedia.org/wiki/Scoring_rule">scoring rules</a>. Given a probability vector <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> with a probability for each of the <span class="math notranslate nohighlight">\(i\)</span> outcomes. A scoring rule will give a reward of <span class="math notranslate nohighlight">\(S({\mathbf {r}}, i)\)</span> if the <span class="math notranslate nohighlight">\(i\)</span>th event occurs. We say we have a proper scoring rule, if the highest expected reward is obtained by reporting the true probability distribution. A proper scoring rule is said to be local if its value depends only on the probability <span class="math notranslate nohighlight">\(r_{i}\)</span>.  It can be <a class="reference external" href="https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf">shown</a> that the logarithmic scoring rule is the only local and proper scoring rule. This is true up to an affine transformation, that is if <span class="math notranslate nohighlight">\(S(\mathbf {r} ,i)\)</span> is a strictly proper scoring rule then <span class="math notranslate nohighlight">\(a+b S({\mathbf {r}},i)\)</span> with <span class="math notranslate nohighlight">\(b&gt;0\)</span> is also a strictly proper scoring rule. Long story short, it is pretty common to use the log-likelihood <span class="math notranslate nohighlight">\(\log p(y_i \mid \theta)\)</span> as a measure of the point-wise predictive accuracy.</p>
<p>When the likelihood is Gaussian, then the average log-likelihood will be proportional to the quadratic mean error. For historical reasons people use the <em>deviance</em> scale when talking about predictive accuracy, this is simply multiplying the log-likelihood by <span class="math notranslate nohighlight">\(-2\)</span>:</p>
<div class="math notranslate nohighlight">
\[-2\ \sum_{i=1}^{n} \log \ p(y_i \mid \theta)\]</div>
<p>The <em>deviance</em> is used in both Bayesians and non-Bayesians context, in the former <span class="math notranslate nohighlight">\(\theta\)</span> is a probability distribution and in the latter a point-estimate.</p>
<blockquote>
<div><p>The lower the deviance, the larger the log-likelihood and thus the greater the agreement between model’s predictions and data. We want smaller values of deviance.</p>
</div></blockquote>
<p>In principle the more complex a model (the more parameters to tune) the lower the deviance will be. This is reflecting the intuition that a model with more parameters will be in general more flexible and thus it will fit the data better. Thus relying only on the deviance could lead us to choose models prone to <a class="reference external" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. Overfitting is the tendency of a model to adjust so well to the data used to fit it that it will be very bad at fitting (or generalizing) to new data. For this reason the deviance is used together with a term penalizing the over-complexity of models.</p>
<div class="section" id="akaike-information-criterion">
<h3>Akaike Information Criterion<a class="headerlink" href="#akaike-information-criterion" title="Permalink to this headline">¶</a></h3>
<p>This is probably the most well known IC, especially for non-Bayesians and is defined as the sum of two terms: The <span class="math notranslate nohighlight">\(\log p(y_i \mid \hat{\theta}_{mle})\)</span>, measures how well the model fits the data and the penalization term <span class="math notranslate nohighlight">\(p_{AIC}\)</span> takes into account that we are using the same data to fit the model and to evaluate the model.</p>
<div class="math notranslate nohighlight">
\[AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{mle}) + 2 p_{AIC} \]</div>
<p>Here <span class="math notranslate nohighlight">\(\hat{\theta}_{mle}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum-likelihood estimation</a> of <span class="math notranslate nohighlight">\(\theta\)</span> and $ p_{AIC}$ is just the number of parameters in the model.</p>
<p>AIC performs well in non-bayesian settings, but is not well equipped to deal with the generality of Bayesian models. It does not use the full <em>a posteriori</em> distribution, discarding potentially useful information. On average AIC will behave worst as we increase the information in the priors, or in general the structure in our model. Thus it is not compatible with informative and weakly informative priors, neither with hierarchical models. AIC assumes that the posterior can be well represented (at least asymptotically) by a Gaussian distribution, but this is not true for a number of models, including hierarchical models, mixture models, neural networks, etc. Fortunately, we have better alternatives.</p>
</div>
</div>
<div class="section" id="widely-available-information-criterion">
<h2>Widely available information criterion<a class="headerlink" href="#widely-available-information-criterion" title="Permalink to this headline">¶</a></h2>
<p>WAIC, generally pronounced as W-A-I-C, even when something like <em>wæɪk</em> is less of a mouthful ;-) can be regarded as a fully Bayesian extension of AIC. It also has two terms, although computed in a different way. The most important difference is that the terms are computed using the full posterior distribution, including the <em>effective</em> number of parameters.</p>
<p>It also has two terms, although computed in a different way, being the most important difference that the terms are computed using the full posterior distribution, including the <em>effective</em> number of parameters. For details on the computation of WAIC please read the WAIC in depth section.</p>
</div>
<div class="section" id="pareto-smoothed-importance-sampling-leave-one-out-cross-validation">
<h2>Pareto smoothed importance sampling leave-one-out cross validation<a class="headerlink" href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">Cross-validation</a> (CV) is another method of estimating out-of-sample prediction accuracy. This method requires re-fitting a model many times, each time excluding a portion of the data, the excluded portion is then used to measure the accuracy of them model. This process is repeated many times and the estimated accuracy of the model will be the average of each run. Then the entire dataset is used to fit the model one more time and this is the model used for further analysis and/or predictions. Leave-one-out cross-validation (LOO-CV) is a particular type of cross-validation when the data excluded is a single data-point.</p>
<p>As CV can be quite time consuming (especially for Bayesian models) it is interesting to note that in theory it is possible to approximate LOO-CV. A practical and computational efficient way to do it requires using a combination of strategies that includes what is called <a class="reference external" href="https://arxiv.org/abs/1507.02646">Pareto smoothed importance sampling</a>. The resulting method is known as PSIS-LOO-CV which, while very useful, has a very complicated name, thus we just call it LOO.</p>
<p>While LOO and WAIC approximate two different quantities, asymptotically they converge to the same numerical value, and also in practice they generally agree. The main advantage of LOO is that it is more informative as it provides <a class="reference external" href="https://arxiv.org/abs/1507.04544">useful diagnostics</a> and other goodies such as effective sample size and Monte Carlo standard error estimates.</p>
<p>Using ArviZ, both LOO and WAIC can be computed just by calling a function. Let’s try on an arbitrary pre-loaded model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># change this to some good example</span>
<span class="n">model0</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s1">&#39;regression1d&#39;</span><span class="p">)</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">load_arviz_data</span><span class="p">(</span><span class="s1">&#39;regression1d&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">waic</span><span class="p">(</span><span class="n">model0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Computed from 2000 by 100 log-likelihood matrix

          Estimate       SE
elpd_waic  -145.85     5.71
p_waic        2.73        -

The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if
you rely on a specific value.
A higher log-score (or a lower deviance) indicates a model with better predictive
accuracy.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">model0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Computed from 2000 by 100 log-likelihood matrix

         Estimate       SE
elpd_loo  -145.85     5.71
p_loo        2.73        -

The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if
you rely on a specific value.
A higher log-score (or a lower deviance) indicates a model with better predictive
accuracy.
</pre></div>
</div>
</div>
</div>
<p>As you can see both WAIC and LOO return similar values. ArviZ comes equipped with the <code class="docutils literal notranslate"><span class="pre">compare(.)</span></code> function. That is more convenient than using <code class="docutils literal notranslate"><span class="pre">loo(.)</span></code> or <code class="docutils literal notranslate"><span class="pre">waic(.)</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">model0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Computed from 2000 by 100 log-likelihood matrix

         Estimate       SE
elpd_loo  -145.85     5.71
p_loo        2.73        -

The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if
you rely on a specific value.
A higher log-score (or a lower deviance) indicates a model with better predictive
accuracy.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-compare-function">
<h2>The compare function<a class="headerlink" href="#the-compare-function" title="Permalink to this headline">¶</a></h2>
<p>This function takes a dictionary of names (keys) and models (values) as input and returns a DataFrame ordered (row-wise) from best to worst model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">cmp</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">({</span><span class="s2">&quot;m0&quot;</span><span class="p">:</span><span class="n">model0</span><span class="p">,</span> <span class="s2">&quot;m1&quot;</span><span class="p">:</span><span class="n">model1</span><span class="p">,})</span>
<span class="nb">cmp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="stderr docutils container">
<pre class="stderr literal-block">/home/osvaldo/proyectos/00_BM/arviz/arviz/stats/stats.py:151: UserWarning: 
The scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if you rely on a specific value.
A higher log-score (or a lower deviance) indicates a model with better predictive accuracy.
  &quot;\nThe scale is now log by default. Use 'scale' argument or &quot;
</pre>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>loo</th>
      <th>p_loo</th>
      <th>d_loo</th>
      <th>weight</th>
      <th>se</th>
      <th>dse</th>
      <th>warning</th>
      <th>loo_scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>m0</th>
      <td>0</td>
      <td>-145.855</td>
      <td>2.7325</td>
      <td>0</td>
      <td>0.5</td>
      <td>5.66942</td>
      <td>0</td>
      <td>False</td>
      <td>log</td>
    </tr>
    <tr>
      <th>m1</th>
      <td>1</td>
      <td>-145.855</td>
      <td>2.7325</td>
      <td>0</td>
      <td>0.5</td>
      <td>5.66942</td>
      <td>0</td>
      <td>False</td>
      <td>log</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We have many columns, so let’s check out their meaning one by one:</p>
<ol class="simple">
<li><p>The index are the names of the models taken from the keys of the dictionary passed to <code class="docutils literal notranslate"><span class="pre">compare(.)</span></code>.</p></li>
<li><p><strong>rank</strong>, the ranking on the models starting from 0 (best model) to the number of models.</p></li>
<li><p><strong>waic</strong>, the values of WAIC/LOO. The DataFrame is always sorted from best WAIC/LOO to worst.</p></li>
<li><p><strong>p_waic</strong>, the value of the penalization term. We can roughly think of this value as the estimated effective number of parameters (but do not take that too seriously).</p></li>
<li><p><strong>d_waic</strong>, the relative difference between the value of WAIC/LOO for the top-ranked model and the value of WAIC/LOO for each model. For this reason we will always get a value of 0 for the first model.</p></li>
<li><p><strong>weight</strong>, the weights assigned to each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. See model averaging section for more details.</p></li>
<li><p><strong>se</strong>, the standard error for the WAIC/LOO computations. The standard error can be useful to assess the uncertainty of the WAIC/LOO estimates. By default these errors are computed using bootstrapping.</p></li>
<li><p><strong>dse</strong>, the standard errors of the difference between two values of WAIC/LOO. The same way that we can compute the standard error for each value of WAIC/LOO, we can compute the standard error of the differences between two values of WAIC/LOO. Notice that both quantities are not necessarily the same, the reason is that the uncertainty about WAIC/LOO is correlated between models. This quantity is always 0 for the top-ranked model.</p></li>
<li><p><strong>warning</strong>, when computing WAIC/LOO, the possible values can be <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>. If <code class="docutils literal notranslate"><span class="pre">True</span></code> the computation of WAIC/LOO may not be reliable. This warning for WAIC is based on an empirical determined cutoff value and need to be interpreted with caution. The warning for LOO has better empirical and theoretical support.</p></li>
<li><p><strong>waic_scale</strong>, the scale of the reported values. The default is the deviance scale as previously mentioned this is obtained by multiplying the log-score by -2. Other options are log – this is the log-score multiplied by 1 (this reverts the order: a higher WAIC/LOO will be better) – and negative-log – this is the log-score multiplied by -1 (as with the deviance scale, a lower value is better).</p></li>
</ol>
</div>
<div class="section" id="the-plot-compare-function">
<h2>The plot_compare function<a class="headerlink" href="#the-plot-compare-function" title="Permalink to this headline">¶</a></h2>
<p>ArviZ also provides another convenience function that takes the output of <code class="docutils literal notranslate"><span class="pre">compare(.)</span></code> and produces a summary plot in the style of the one used in the book Statistical Rethinking by Richard McElreath.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="nb">cmp</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Model_selection_14_0.png" src="../../_images/Model_selection_14_0.png" />
</div>
</div>
<p>The empty circle represents the values of WAIC/LOO and the black error bars associated with them are the values of the standard deviation of WAIC/LOO.</p>
<p>The value of the best WAIC/LOO is also indicated with a vertical dashed grey line to ease comparison with other WAIC/LOO values.</p>
<p>The filled black dots are the in-sample deviance of each model, i.e. the log-score without the penalty term.</p>
<p>For all models except the top-ranked one we also get a triangle indicating the value of the difference of WAIC between that model and the top model and a grey errorbar indicating the standard error of the differences between the top-ranked WAIC/LOO and WAIC/LOO for each model.</p>
</div>
<div class="section" id="point-wise-model-comparison">
<h2>Point-wise model comparison<a class="headerlink" href="#point-wise-model-comparison" title="Permalink to this headline">¶</a></h2>
<p>Comparing models is a good way to get a better understanding about them…</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ArviZ-devs<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>