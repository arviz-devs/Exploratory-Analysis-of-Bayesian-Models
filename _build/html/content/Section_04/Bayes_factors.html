

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Bayes factors &#8212; Exploratory analysis of Bayesian models</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Exploratory analysis of Bayesian models</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Section_04/Bayes_factors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#all-that-glitter-is-not-gold" class="nav-link">All that glitter is not gold</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#bayes-factors-vs-information-criteria" class="nav-link">Bayes Factors vs Information Criteria</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayes-factors">
<h1>Bayes factors<a class="headerlink" href="#bayes-factors" title="Permalink to this headline">¶</a></h1>
<p>Bayes factors are a common alternative to information criteria. And a subject that generally acts as a dividing line between Bayesians: We have those who use Bayes factors and those who dislike them. I mean, I bet there are people in the middle too, but more often than not this is a polarizing topic.</p>
<p>To understand Bayes factors let us begin by writing Bayes theorem in a way that explicitly shows that our inferences are always model dependent.</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid y, M_k) = {\frac {p(y \mid \theta, M_k)p(\theta \mid M_k)}{p(y \mid M_k)}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(y\)</span> represents the data and <span class="math notranslate nohighlight">\(M\)</span> the model:</p>
<p>The term in the denominator is known as marginal likelihood (or evidence). When doing inference we do not need to compute this normalizing constant, so in practice we often compute the posterior up to a constant factor. However, for model comparison and model averaging the marginal likelihood becomes a relevant quantity. If our main objective is to choose only one model, the best one, from a set of <span class="math notranslate nohighlight">\(k\)</span> models we can just choose the one with the largest <span class="math notranslate nohighlight">\(p(y \mid M_k)\)</span>. As a general rule <span class="math notranslate nohighlight">\(p(y \mid M_k)\)</span> are tiny numbers and do not tell us too much on their own; like with information criteria, what matters are the relative values. So in practice people often compute the ratio of two marginal likelihoods, and this is called a Bayes factor:</p>
<div class="math notranslate nohighlight">
\[BF = \frac{p(y \mid M_0)}{p(y \mid M_1)}\]</div>
<p>Then when BF &gt; 1, model 0 explains data better than model 1.</p>
<p>Some authors have proposed tables with ranges to discretize and ease BF interpretation. The following table indicates the strength of the evidence favoring model 0 against model 1:</p>
<ul class="simple">
<li><p>1-3: anecdotal</p></li>
<li><p>3-10: moderate</p></li>
<li><p>10-30: strong</p></li>
<li><p>30-100: very strong</p></li>
<li><p>&gt; 100: extreme</p></li>
</ul>
<p>Remember, these rules are just conventions, simple guides at best. But results should always be put into context and should be accompanied with enough detail so others could potentially check if they agree with our conclusions. The strength of the evidence necessary to make a claim is not the same in particle physics, a court, or to evacuate a town and prevent hundreds of deaths.</p>
<p>Using <span class="math notranslate nohighlight">\(p(y \mid M_k)\)</span> to compare model is totally fine if all models are assumed to have the same prior probability<a class="reference external" href="#fn1"><sup>1</sup></a>. Otherwise, we have to compute the <em>posterior odds</em>:</p>
<div class="math notranslate nohighlight">
\[\underbrace{\frac{p(M_0 \mid y)}{p(M_1 \mid y)}}_\text{posterior odds} = \underbrace{\frac{p(y \mid M_0)}{p(y \mid M_1)}}_\text{Bayes factors} \, \underbrace{\frac{p(\ M_0 \ )}{p(\ M_1 \ )}}_\text{prior odds}\]</div>
<p><span id="fn1"> <sup>1</sup> Notice that we are talking about the prior probability we assign to models and not about the priors we assign to parameters for each model</span></p>
<div class="section" id="all-that-glitter-is-not-gold">
<h2>All that glitter is not gold<a class="headerlink" href="#all-that-glitter-is-not-gold" title="Permalink to this headline">¶</a></h2>
<p>Now we will briefly discuss some key facts about the marginal likelihood. By carefully inspecting the definition of marginal likelihood we can understand their properties and consequences for their practical use:</p>
<div class="math notranslate nohighlight">
\[p(y \mid M_k) = \int_{\theta_k} p(y \mid \theta_k, M_k) p(\theta_k, M_k) d\theta_k\]</div>
<ul class="simple">
<li><p><strong>The good: Models with more parameters have a larger penalization than models with fewer parameters</strong>. Bayes factors has a built-in Occam Razor! The intuitive reason is that the larger the number of parameters the more <em>spread-out</em> the prior will be with respect to the likelihood. Or in other words a more <em>spread out</em> prior is one that admits as plausible more datasets than a more concentrated one. This will be reflected in the computation of the above integral as you will get a smaller value with a wider prior than with a more concentrated prior.</p></li>
<li><p><strong>The bad: Computing the marginal likelihood is, generally, a hard task</strong>. The above is an integral of a highly variable function over a high dimensional parameter space. In general this integral needs to be solved numerically using more or less sophisticated methods (see and example <a class="reference external" href="https://docs.pymc.io/notebooks/Bayes_factor.html">here</a>).</p></li>
<li><p><strong>The ugly: The marginal likelihood depends <em>sensitively</em> on the values of the priors</strong>. Using the marginal likelihood to compare models is a good idea because a penalization for complex models is already included (thus preventing us from overfitting). At the same time, a change in the prior will affect the computations of the marginal likelihood. At first this sounds a little bit silly: we already know that priors affect computations (otherwise we could simply avoid them), but the point here is the word “sensitively”. We are talking about changes in the prior that will keep inference of the parameters <em>more or less</em> the same, but could have a big impact on the value of the marginal likelihood. Another source of criticism to Bayes factors is that they can be used as a Bayesian way of doing hypothesis testing; there is  nothing wrong with this <em>per se</em>, but many authors have pointed out that an inference approach, similar to the one used in this book, is better suited to  most problems than the generally taught approach of hypothesis testing (whether  Bayesian or not Bayesian).</p></li>
</ul>
</div>
<div class="section" id="bayes-factors-vs-information-criteria">
<h2>Bayes Factors vs Information Criteria<a class="headerlink" href="#bayes-factors-vs-information-criteria" title="Permalink to this headline">¶</a></h2>
<p>WAIC/LOO uses the log-likelihood and the priors are not directly part of the computations – they are indirectly included because they will have an effect on the values of the models parameters. Instead, Bayes factors use priors directly as we need to average the likelihood over the prior. Conceptually we can say that Bayes factors are focused on identifying the best model (and the prior is part of the model) while WAIC and LOO are focused on which parameters will give the best predictions. In most scenarios priors are used for their regularizing properties and, when possible, to provide some background knowledge – more so than because we <em>really really believe</em> they reflect some <em>truth</em>. As a result, we think information criteria are a more robust approach in practice. Moreover, their computation is far less problematic and generally more robust, without the need to use special samplers or methods.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ArviZ-devs<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>